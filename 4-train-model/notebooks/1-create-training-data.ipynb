{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_preprocessing.py\n",
    "from datetime import date, datetime, timedelta\n",
    "import polars as pl\n",
    "from deltalake import DeltaTable\n",
    "import json\n",
    "from gcsfs import GCSFileSystem\n",
    "import pyarrow\n",
    "\n",
    "# custom imports\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.gcp import GCPClient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataPreprocessor:\n",
    "    def __init__(self, gcp_client, bucket: str, table_path: str):\n",
    "        self.gcp_client = gcp_client\n",
    "        self.table_path = table_path\n",
    "        self.bucket = bucket\n",
    "        self.table_path_full = f'gs://{bucket}/{table_path}'\n",
    "    \n",
    "    def get_date_from_lookback(self, lookback_days: int, return_as_str = True):\n",
    "        target_date = datetime.utcnow() - timedelta(days=lookback_days)\n",
    "        \n",
    "        if return_as_str:\n",
    "            year, month, day = target_date.year, target_date.month, target_date.day\n",
    "            return f'{year}/{month:02d}/{day:02d}'\n",
    "        else:\n",
    "            return target_date\n",
    "\n",
    "    def read_data(self):\n",
    "\n",
    "        start_date = self.get_date_from_lookback(lookback_days=1000, return_as_str=True)\n",
    "        print(f\"Reading data from {self.table_path_full} with start date {start_date}\")\n",
    "\n",
    "        gcp_creds_json_str = json.dumps(self.gcp_client.creds_json)\n",
    "        storage_options = {\"service_account_key\": gcp_creds_json_str}\n",
    "        \n",
    "        dt = DeltaTable(self.table_path_full, storage_options=storage_options)\n",
    "        # dt = DeltaTable(self.table_path_full, storage_options=storage_options)\n",
    "\n",
    "        gcp_client = GCPClient()   \n",
    "        fs = pyarrow.fs.GcsFileSystem(access_token=gcp_client.creds_encoded, credential_token_expiration = datetime.fromisoformat('9999-12-31') )\n",
    "\n",
    "        df = dt.to_pyarrow_dataset(partitions=[(\"crt_ts_date\", \">=\", start_date)])\n",
    "        # df = pl.scan_pyarrow_dataset(df)\n",
    "\n",
    "        \n",
    "        # df = pl.scan_delta(\n",
    "        #         self.table_path_full,\n",
    "        #         pyarrow_options={\"partitions\": [(\"crt_ts_date\", \">=\", start_date)]},\n",
    "        #         storage_options=storage_options,\n",
    "        # )\n",
    "        \n",
    "        return df\n",
    "\n",
    "\n",
    "    # Primary data processing\n",
    "    \n",
    "    def create_target(self, df):\n",
    "        # Create target variable, time to landing\n",
    "        landing_times = df.group_by('fa_flight_id').agg(pl.max('actual_in').alias('actual_in_filled'))\n",
    "        df = df.join(landing_times, on='fa_flight_id')\n",
    "        df = df.with_columns( ((pl.col('actual_in_filled').dt.timestamp(\"ms\")-pl.col('event_ts').dt.timestamp(\"ms\"))/1000/60/60 ).alias('target') )\n",
    "        \n",
    "        df = df.sort(['actual_in_filled','crt_ts'])\n",
    "        return df\n",
    "\n",
    "    def remove_incomplete_flights(self, df):\n",
    "        # Remove flights that haven't landed yet\n",
    "        return df.filter( pl.col('actual_in_filled').is_not_null() )\n",
    "\n",
    "    def removed_arrival_events(self, df):\n",
    "        # Arrival events are not useful for training\n",
    "        return df.filter(pl.col('event_type') != 'actual_in')\n",
    "\n",
    "    def process_data(self, df):\n",
    "        df = self.create_target(df)\n",
    "        df = self.remove_incomplete_flights(df)\n",
    "        df = self.removed_arrival_events(df)\n",
    "        return df\n",
    "    \n",
    "    def write_data_to_gcs(self, df, path_out: str):\n",
    "        print(f\"Writing data to {path_out}\")\n",
    "        bucket = self.gcp_client.storage_client.get_bucket(self.bucket) \n",
    "        blob = bucket.blob(f'{path_out}.csv')\n",
    "        blob.upload_from_string(df.collect().write_csv(), 'text/csv')\n",
    "\n",
    "    def write_data_locally(self, df, path_out: str):\n",
    "        print(f\"Writing data to {path_out}\")\n",
    "        df.collect().write_parquet(path_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcp_client = GCPClient()\n",
    "\n",
    "project_id = 'aia-ds-accelerator-flight-1'\n",
    "bucket = 'datalake-flight-dev-1'\n",
    "table_path_in = 'flightsummary-delta-processed-stream'\n",
    "table_path_out = 'flightsummary-delta-processed-training'\n",
    "\n",
    "data_preprocessor = DataPreprocessor(gcp_client, bucket=bucket, table_path=table_path_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from gs://datalake-flight-dev-1/flightsummary-delta-processed-stream with start date 2021/02/10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<i>naive plan: (run <b>LazyFrame.explain(optimized=True)</b> to see the optimized plan)</i>\n",
       "    <p></p>\n",
       "    <div><p></p>  PYTHON SCAN <p></p>  PROJECT */76 COLUMNS</div>"
      ],
      "text/plain": [
       "<LazyFrame [76 cols, {\"ident\": Utf8 â€¦ \"event_ts\": Datetime(time_unit='us', time_zone=None)}] at 0x10572F0A0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = data_preprocessor.read_data()\n",
    "df = pl.scan_pyarrow_dataset(df)\n",
    "df = data_preprocessor.process_data(df)\n",
    "data_preprocessor.write_data_to_gcs(df, table_path_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 3: Write data\n",
    "data_preprocessor.write_data_to_gcs(df_processed, path_out=table_path_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
